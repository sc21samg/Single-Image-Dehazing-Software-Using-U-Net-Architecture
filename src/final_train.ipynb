{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtbhKRWIoNBf",
        "outputId": "9f1d6422-01ce-45ce-ec7c-2bb60a0b2161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 256, 256, 64)         1792      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 64)                   0         ['activation_1[0][0]']        \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 1, 1, 64)             0         ['global_average_pooling2d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 1, 1, 4)              256       ['reshape[0][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1, 1, 64)             256       ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 256, 256, 64)         0         ['activation_1[0][0]',        \n",
            "                                                                     'dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['multiply[0][0]']            \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 128)                  0         ['activation_3[0][0]']        \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)         (None, 1, 1, 128)            0         ['global_average_pooling2d_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 1, 1, 8)              1024      ['reshape_1[0][0]']           \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 1, 1, 128)            1024      ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)       (None, 128, 128, 128)        0         ['activation_3[0][0]',        \n",
            "                                                                     'dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['multiply_1[0][0]']          \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " global_average_pooling2d_2  (None, 256)                  0         ['activation_5[0][0]']        \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)         (None, 1, 1, 256)            0         ['global_average_pooling2d_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 1, 1, 16)             4096      ['reshape_2[0][0]']           \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 1, 1, 256)            4096      ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)       (None, 64, 64, 256)          0         ['activation_5[0][0]',        \n",
            "                                                                     'dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['multiply_2[0][0]']          \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " global_average_pooling2d_3  (None, 512)                  0         ['activation_7[0][0]']        \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)         (None, 1, 1, 512)            0         ['global_average_pooling2d_3[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 1, 1, 32)             16384     ['reshape_3[0][0]']           \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 1, 1, 512)            16384     ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)       (None, 32, 32, 512)          0         ['activation_7[0][0]',        \n",
            "                                                                     'dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 32, 32, 512)          0         ['multiply_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['dropout[0][0]']             \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 1024)                 0         ['activation_9[0][0]']        \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)         (None, 1, 1, 1024)           0         ['global_average_pooling2d_4[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 1, 1, 64)             65536     ['reshape_4[0][0]']           \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 1, 1, 1024)           65536     ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)       (None, 16, 16, 1024)         0         ['activation_9[0][0]',        \n",
            "                                                                     'dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 16, 16, 1024)         0         ['multiply_4[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, 32, 32, 512)          2097664   ['dropout_1[0][0]']           \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 32, 32, 1024)         0         ['conv2d_transpose[0][0]',    \n",
            "                                                                     'multiply_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 32, 32, 512)          4719104   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_5  (None, 512)                  0         ['activation_11[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " reshape_5 (Reshape)         (None, 1, 1, 512)            0         ['global_average_pooling2d_5[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 1, 1, 32)             16384     ['reshape_5[0][0]']           \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 1, 1, 512)            16384     ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)       (None, 32, 32, 512)          0         ['activation_11[0][0]',       \n",
            "                                                                     'dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, 64, 64, 256)          524544    ['multiply_5[0][0]']          \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 64, 64, 512)          0         ['conv2d_transpose_1[0][0]',  \n",
            " )                                                                   'multiply_2[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 64, 64, 256)          1179904   ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_6  (None, 256)                  0         ['activation_13[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " reshape_6 (Reshape)         (None, 1, 1, 256)            0         ['global_average_pooling2d_6[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 1, 1, 16)             4096      ['reshape_6[0][0]']           \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 1, 1, 256)            4096      ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)       (None, 64, 64, 256)          0         ['activation_13[0][0]',       \n",
            "                                                                     'dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 128, 128, 128)        131200    ['multiply_6[0][0]']          \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_transpose_2[0][0]',  \n",
            " )                                                                   'multiply_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 128, 128, 128)        295040    ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_7  (None, 128)                  0         ['activation_15[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " reshape_7 (Reshape)         (None, 1, 1, 128)            0         ['global_average_pooling2d_7[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 1, 1, 8)              1024      ['reshape_7[0][0]']           \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 1, 1, 128)            1024      ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)       (None, 128, 128, 128)        0         ['activation_15[0][0]',       \n",
            "                                                                     'dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 256, 256, 64)         32832     ['multiply_7[0][0]']          \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 256, 256, 128)        0         ['conv2d_transpose_3[0][0]',  \n",
            " )                                                                   'multiply[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 256, 256, 64)         73792     ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_8  (None, 64)                   0         ['activation_17[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " reshape_8 (Reshape)         (None, 1, 1, 64)             0         ['global_average_pooling2d_8[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 1, 1, 4)              256       ['reshape_8[0][0]']           \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 1, 1, 64)             256       ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)       (None, 256, 256, 64)         0         ['activation_17[0][0]',       \n",
            "                                                                     'dense_17[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 256, 256, 3)          195       ['multiply_8[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31273539 (119.30 MB)\n",
            "Trainable params: 31261763 (119.25 MB)\n",
            "Non-trainable params: 11776 (46.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate, Activation,\n",
        "                                     BatchNormalization, GlobalAveragePooling2D, Reshape, Dense, multiply,\n",
        "                                     Dropout)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.losses import MSE, MeanSquaredError\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Load VGG19 model for perceptual loss, excluding the top fully connected layers\n",
        "vgg = VGG19(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
        "vgg.trainable = False\n",
        "\n",
        "# Select the output layer for feature extraction\n",
        "output_layer = vgg.get_layer('block2_conv2').output\n",
        "vgg_model = Model(inputs=vgg.input, outputs=output_layer)\n",
        "\n",
        "def perceptual_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the perceptual loss between y_true and y_pred using the VGG19 model.\n",
        "    \"\"\"\n",
        "    vgg_true = vgg_model(y_true)\n",
        "    vgg_pred = vgg_model(y_pred)\n",
        "    return K.mean(K.square(vgg_true - vgg_pred))\n",
        "\n",
        "\n",
        "# Initialize Mean Squared Error loss instance\n",
        "mse_loss = MeanSquaredError()\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom loss function that combines MSE and perceptual loss,\n",
        "    with explicit type casting for mixed precision compatibility.\n",
        "    \"\"\"\n",
        "    mse = mse_loss(y_true, y_pred)\n",
        "    p_loss = perceptual_loss(y_true, y_pred)\n",
        "\n",
        "    # Ensure both mse and p_loss are float32 before addition\n",
        "    mse = tf.cast(mse, 'float32')\n",
        "    p_loss = tf.cast(p_loss, 'float32')\n",
        "\n",
        "    return mse + p_loss\n",
        "\n",
        "\n",
        "# Squeeze and Excite block\n",
        "def squeeze_excite_block(input_tensor, ratio=16):\n",
        "    init = input_tensor\n",
        "    channel_axis = -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = multiply([init, se])\n",
        "    return x\n",
        "\n",
        "# Define a convolutional block\n",
        "def conv_block(x, num_filters):\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Build U-Net model incorporating Squeeze and Excite blocks and adding Bottleneck with Dropout\n",
        "def build_unet(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Contracting Path (Encoder)\n",
        "    c1 = conv_block(inputs, 64)\n",
        "    c1 = squeeze_excite_block(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = conv_block(p1, 128)\n",
        "    c2 = squeeze_excite_block(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = conv_block(p2, 256)\n",
        "    c3 = squeeze_excite_block(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = conv_block(p3, 512)\n",
        "    c4 = squeeze_excite_block(c4)\n",
        "    d4 = Dropout(0.5)(c4)\n",
        "    p4 = MaxPooling2D((2, 2))(d4)\n",
        "\n",
        "    # Bottleneck with dropout\n",
        "    c5 = conv_block(p4, 1024)\n",
        "    c5 = squeeze_excite_block(c5)\n",
        "    c5 = Dropout(0.5)(c5)  # Adding dropout in the bottleneck\n",
        "\n",
        "    # Expansive Path (Decoder)\n",
        "    u4 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u4 = Concatenate()([u4, c4])\n",
        "    c6 = conv_block(u4, 512)\n",
        "    c6 = squeeze_excite_block(c6)\n",
        "\n",
        "    u3 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u3 = Concatenate()([u3, c3])\n",
        "    c7 = conv_block(u3, 256)\n",
        "    c7 = squeeze_excite_block(c7)\n",
        "\n",
        "    u2 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u2 = Concatenate()([u2, c2])\n",
        "    c8 = conv_block(u2, 128)\n",
        "    c8 = squeeze_excite_block(c8)\n",
        "\n",
        "    u1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u1 = Concatenate()([u1, c1])\n",
        "    c9 = conv_block(u1, 64)\n",
        "    c9 = squeeze_excite_block(c9)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(3, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Metrics\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    # Peak Signal-to-Noise Ratio metric\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def ssim_metric(y_true, y_pred):\n",
        "    # Structural Similarity Index\n",
        "    y_true_f32 = tf.cast(y_true, tf.float32)\n",
        "    y_pred_f32 = tf.cast(y_pred, tf.float32)\n",
        "    return tf.image.ssim(y_true_f32, y_pred_f32, max_val=1.0)\n",
        "\n",
        "# Example usage\n",
        "model = build_unet((256, 256, 3))\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=[psnr_metric, ssim_metric])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Aub0SZSfOuE",
        "outputId": "b2607c13-216e-410f-9820-568db31c8d3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Starting dataset creation...\n",
            "Collecting image paths...\n",
            "Collected 2990 pairs of images.\n",
            "Creating dataset from paths...\n",
            "Processing: Tensor(\"args_0:0\", shape=(), dtype=string) and Tensor(\"args_1:0\", shape=(), dtype=string)\n",
            "Shuffling dataset...\n",
            "Batching dataset...\n",
            "Dataset created and ready for use.\n",
            "Sample batch - Hazy images shape: (32, 256, 256, 3), dtype: <dtype: 'float32'>\n",
            "Sample batch - Clear images shape: (32, 256, 256, 3), dtype: <dtype: 'float32'>\n",
            "Starting Epoch 1\n",
            "Epoch 1/30\n",
            "5/5 [==============================] - 31s 1s/step - loss: 6.0013 - psnr_metric: 9.9020 - ssim_metric: 0.1627 - val_loss: 7.2336 - val_psnr_metric: 9.9580 - val_ssim_metric: 0.3294\n",
            "Starting Epoch 2\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 4.3820 - psnr_metric: 10.9774 - ssim_metric: 0.2393 - val_loss: 7.1432 - val_psnr_metric: 10.0098 - val_ssim_metric: 0.3353\n",
            "Starting Epoch 3\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 3.2878 - psnr_metric: 11.8653 - ssim_metric: 0.3036 - val_loss: 7.0206 - val_psnr_metric: 10.0647 - val_ssim_metric: 0.3441\n",
            "Starting Epoch 4\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 2.5793 - psnr_metric: 12.4668 - ssim_metric: 0.3486 - val_loss: 6.8847 - val_psnr_metric: 10.1066 - val_ssim_metric: 0.3548\n",
            "Starting Epoch 5\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 2.1564 - psnr_metric: 12.9692 - ssim_metric: 0.3873 - val_loss: 6.7307 - val_psnr_metric: 10.1370 - val_ssim_metric: 0.3654\n",
            "Starting Epoch 6\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 1s 166ms/step - loss: 1.8282 - psnr_metric: 13.4321 - ssim_metric: 0.4187 - val_loss: 6.5684 - val_psnr_metric: 10.1561 - val_ssim_metric: 0.3752\n",
            "Starting Epoch 7\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 1.5990 - psnr_metric: 13.9484 - ssim_metric: 0.4443 - val_loss: 6.3981 - val_psnr_metric: 10.1750 - val_ssim_metric: 0.3844\n",
            "Starting Epoch 8\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 1.4550 - psnr_metric: 14.3891 - ssim_metric: 0.4647 - val_loss: 6.2358 - val_psnr_metric: 10.2018 - val_ssim_metric: 0.3937\n",
            "Starting Epoch 9\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 1.3426 - psnr_metric: 14.7387 - ssim_metric: 0.4828 - val_loss: 6.0816 - val_psnr_metric: 10.2310 - val_ssim_metric: 0.4021\n",
            "Starting Epoch 10\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 1.2437 - psnr_metric: 15.1857 - ssim_metric: 0.4977 - val_loss: 5.9550 - val_psnr_metric: 10.2798 - val_ssim_metric: 0.4091\n",
            "Starting Epoch 11\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 1.1787 - psnr_metric: 15.3648 - ssim_metric: 0.5093 - val_loss: 5.8193 - val_psnr_metric: 10.3404 - val_ssim_metric: 0.4160\n",
            "Starting Epoch 12\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 1.1210 - psnr_metric: 15.4495 - ssim_metric: 0.5205 - val_loss: 5.6688 - val_psnr_metric: 10.4033 - val_ssim_metric: 0.4229\n",
            "Starting Epoch 13\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 1.0640 - psnr_metric: 15.6734 - ssim_metric: 0.5321 - val_loss: 5.5120 - val_psnr_metric: 10.4749 - val_ssim_metric: 0.4300\n",
            "Starting Epoch 14\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 1.0172 - psnr_metric: 15.9035 - ssim_metric: 0.5438 - val_loss: 5.3819 - val_psnr_metric: 10.5510 - val_ssim_metric: 0.4358\n",
            "Starting Epoch 15\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 0.9783 - psnr_metric: 16.0440 - ssim_metric: 0.5503 - val_loss: 5.2600 - val_psnr_metric: 10.6276 - val_ssim_metric: 0.4412\n",
            "Starting Epoch 16\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.9475 - psnr_metric: 16.1241 - ssim_metric: 0.5567 - val_loss: 5.1514 - val_psnr_metric: 10.7148 - val_ssim_metric: 0.4459\n",
            "Starting Epoch 17\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 0.9064 - psnr_metric: 16.3841 - ssim_metric: 0.5658 - val_loss: 5.0246 - val_psnr_metric: 10.8029 - val_ssim_metric: 0.4513\n",
            "Starting Epoch 18\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 0.8641 - psnr_metric: 16.4482 - ssim_metric: 0.5738 - val_loss: 4.9092 - val_psnr_metric: 10.8842 - val_ssim_metric: 0.4561\n",
            "Starting Epoch 19\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 0.8568 - psnr_metric: 16.6584 - ssim_metric: 0.5795 - val_loss: 4.8276 - val_psnr_metric: 10.9704 - val_ssim_metric: 0.4598\n",
            "Starting Epoch 20\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 0.8254 - psnr_metric: 16.7439 - ssim_metric: 0.5831 - val_loss: 4.7191 - val_psnr_metric: 11.0762 - val_ssim_metric: 0.4656\n",
            "Starting Epoch 21\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 0.7818 - psnr_metric: 16.9147 - ssim_metric: 0.5964 - val_loss: 4.6268 - val_psnr_metric: 11.1636 - val_ssim_metric: 0.4693\n",
            "Starting Epoch 22\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 0.7911 - psnr_metric: 16.9518 - ssim_metric: 0.5994 - val_loss: 4.4961 - val_psnr_metric: 11.2738 - val_ssim_metric: 0.4762\n",
            "Starting Epoch 23\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.7531 - psnr_metric: 17.1117 - ssim_metric: 0.5989 - val_loss: 4.3953 - val_psnr_metric: 11.3617 - val_ssim_metric: 0.4809\n",
            "Starting Epoch 24\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.7140 - psnr_metric: 17.2453 - ssim_metric: 0.6074 - val_loss: 4.3149 - val_psnr_metric: 11.4637 - val_ssim_metric: 0.4850\n",
            "Starting Epoch 25\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 0.7251 - psnr_metric: 17.1916 - ssim_metric: 0.6078 - val_loss: 4.2231 - val_psnr_metric: 11.5573 - val_ssim_metric: 0.4900\n",
            "Starting Epoch 26\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 1s 165ms/step - loss: 0.6616 - psnr_metric: 17.5961 - ssim_metric: 0.6182 - val_loss: 4.1609 - val_psnr_metric: 11.6540 - val_ssim_metric: 0.4926\n",
            "Starting Epoch 27\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 0.6551 - psnr_metric: 17.5450 - ssim_metric: 0.6236 - val_loss: 4.0840 - val_psnr_metric: 11.7380 - val_ssim_metric: 0.4966\n",
            "Starting Epoch 28\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 0.6156 - psnr_metric: 17.6831 - ssim_metric: 0.6284 - val_loss: 3.9950 - val_psnr_metric: 11.8645 - val_ssim_metric: 0.5017\n",
            "Starting Epoch 29\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 0.6017 - psnr_metric: 17.6454 - ssim_metric: 0.6300 - val_loss: 3.8804 - val_psnr_metric: 12.0300 - val_ssim_metric: 0.5089\n",
            "Starting Epoch 30\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.5808 - psnr_metric: 17.9200 - ssim_metric: 0.6404 - val_loss: 3.8111 - val_psnr_metric: 12.1350 - val_ssim_metric: 0.5119\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3.8111 - psnr_metric: 12.1350 - ssim_metric: 0.5119\n",
            "Validation Loss: 3.8110620975494385, Validation PSNR: 12.135025024414062, Validation SSIM: 0.5119234323501587\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "def decode_img(img, image_size=(256, 256)):\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, image_size)\n",
        "    img /= 255.0  # Normalize to [0,1]\n",
        "    return img\n",
        "\n",
        "def process_path(clear_path, hazy_path):\n",
        "    print(f\"Processing: {clear_path} and {hazy_path}\")  # Print the paths being processed\n",
        "    clear_img = tf.io.read_file(clear_path)\n",
        "    clear_img = decode_img(clear_img)\n",
        "    hazy_img = tf.io.read_file(hazy_path)\n",
        "    hazy_img = decode_img(hazy_img)\n",
        "    return hazy_img, clear_img\n",
        "\n",
        "def create_dataset(dir_pairs, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    clear_paths, hazy_paths = [], []\n",
        "\n",
        "    print(\"Collecting image paths...\")\n",
        "    for clear_dir, hazy_dir in dir_pairs:\n",
        "        # Check if directories exist\n",
        "        if not os.path.exists(clear_dir):\n",
        "            print(f\"Directory does not exist: {clear_dir}\")\n",
        "            continue\n",
        "        if not os.path.exists(hazy_dir):\n",
        "            print(f\"Directory does not exist: {hazy_dir}\")\n",
        "            continue\n",
        "\n",
        "        for file_name in sorted(os.listdir(clear_dir)):\n",
        "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            base_name = file_name.rsplit('.', 1)[0]\n",
        "            pattern = f\"{base_name}_*\"\n",
        "            hazy_files = [f for f in os.listdir(hazy_dir) if f.startswith(pattern) and f.lower().endswith('.png')]\n",
        "\n",
        "            for hazy_file in hazy_files:\n",
        "                clear_paths.append(os.path.join(clear_dir, file_name))\n",
        "                hazy_paths.append(os.path.join(hazy_dir, hazy_file))\n",
        "\n",
        "    if not clear_paths:\n",
        "        print(\"No image pairs were collected.\")\n",
        "        return None\n",
        "\n",
        "def create_dataset(dir_pairs, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    clear_paths, hazy_paths = [], []\n",
        "\n",
        "    print(\"Collecting image paths...\")\n",
        "    for clear_dir, hazy_dir in dir_pairs:\n",
        "        if not os.path.exists(clear_dir) or not os.path.exists(hazy_dir):\n",
        "            print(f\"Directory not found: {clear_dir} or {hazy_dir}\")\n",
        "            continue\n",
        "\n",
        "        clear_files = os.listdir(clear_dir)\n",
        "        hazy_files = os.listdir(hazy_dir)\n",
        "\n",
        "        if not clear_files or not hazy_files:\n",
        "            print(f\"No files found in directories: {clear_dir} or {hazy_dir}\")\n",
        "            continue\n",
        "\n",
        "        for file_name in sorted(clear_files):\n",
        "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            base_name = file_name.split('.')[0]\n",
        "            matched_hazy_files = [f for f in hazy_files if f.startswith(base_name + \"_\") and f.lower().endswith('.png')]\n",
        "\n",
        "            if not matched_hazy_files:\n",
        "                print(f\"No hazy images matched for {file_name}\")\n",
        "                continue\n",
        "\n",
        "            for hazy_file in matched_hazy_files:\n",
        "                clear_paths.append(os.path.join(clear_dir, file_name))\n",
        "                hazy_paths.append(os.path.join(hazy_dir, hazy_file))\n",
        "    if not clear_paths:\n",
        "        print(\"No image pairs were collected.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Collected {len(clear_paths)} pairs of images.\")\n",
        "\n",
        "    # Create a tf.data.Dataset from paths\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((clear_paths, hazy_paths))\n",
        "    print(\"Creating dataset from paths...\")\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        print(\"Shuffling dataset...\")\n",
        "        dataset = dataset.shuffle(buffer_size=len(clear_paths))\n",
        "\n",
        "    print(\"Batching dataset...\")\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    print(\"Dataset created and ready for use.\")\n",
        "    return dataset\n",
        "\n",
        "# Define the pairs of directories\n",
        "dir_pairs = [\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/reside_clear', '/content/drive/My Drive/diss/myproj/data_train/reside_haze')\n",
        "]\n",
        "\n",
        "\n",
        "# Create and use the TensorFlow dataset\n",
        "print(\"Starting dataset creation...\")\n",
        "dataset = create_dataset(dir_pairs)\n",
        "\n",
        "# Display some info about the dataset (optional)\n",
        "for hazy_images, clear_images in dataset.take(1):\n",
        "    print(f\"Sample batch - Hazy images shape: {hazy_images.shape}, dtype: {hazy_images.dtype}\")\n",
        "    print(f\"Sample batch - Clear images shape: {clear_images.shape}, dtype: {clear_images.dtype}\")\n",
        "\n",
        "#####################\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Clear any previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Set the mixed precision policy\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from dehaze import model, build_unet, my_loss, psnr_metric, ssim_metric  # Adjust import according to your file structure\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "#from dataset import hazy_images, clear_images_matched  # Assuming these are loaded and prepared as shown\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.image import psnr, ssim\n",
        "\n",
        "# Assuming `dataset` is your complete dataset returned from `create_dataset`\n",
        "# First, let's count the number of items in the dataset\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "full_dataset = dataset.shuffle(buffer_size=dataset_size)\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "val_dataset = full_dataset.skip(train_size)\n",
        "\n",
        "# Continue with your model definition and training as before\n",
        "# Ensure you use `train_dataset` and `val_dataset` for training and validation, respectively.\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset_from_paths(hazy_paths, clear_paths, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_paths, clear_paths))\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(hazy_paths))\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Load and prepare your data\n",
        "# Ensure datasets are TensorFlow Dataset objects correctly batched\n",
        "def prepare_tf_dataset(hazy_images, clear_images, batch_size=6):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_images, clear_images))\n",
        "    dataset = dataset.shuffle(buffer_size=len(hazy_images)).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(hazy_images))\n",
        "train_hazy, train_clear = hazy_images[:train_size], clear_images[:train_size]\n",
        "val_hazy, val_clear = hazy_images[train_size:], clear_images[train_size:]\n",
        "\n",
        "# Prepare TensorFlow datasets\n",
        "train_dataset = prepare_tf_dataset(train_hazy, train_clear, batch_size=6)\n",
        "val_dataset = prepare_tf_dataset(val_hazy, val_clear, batch_size=6)\n",
        "\n",
        "\n",
        "# Assume load_datasets returns properly prepared and normalized TensorFlow Dataset objects\n",
        "#train_dataset, val_dataset = load_datasets()\n",
        "\n",
        "# Build the model\n",
        "model = build_unet((256, 256, 3))\n",
        "\n",
        "# Define a learning rate schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True)\n",
        "\n",
        "# Compile the model using Mean Squared Error (MSE) as the loss function\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
        "              loss=custom_loss,  # Using built-in MSE loss\n",
        "              metrics=[psnr_metric, ssim_metric])\n",
        "\n",
        "\n",
        "\n",
        "# Custom callback for epoch printing\n",
        "class TrainingPrint(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"Starting Epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='min')\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = 30,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[ early_stopping_callback, TrainingPrint()])\n",
        "\n",
        "\n",
        "# Save the final model\n",
        "model.save('/content/drive/My Drive/diss/myproj/results/final_train.keras')\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "val_loss, val_psnr, val_ssim = model.evaluate(val_dataset)\n",
        "print(f\"Validation Loss: {val_loss}, Validation PSNR: {val_psnr}, Validation SSIM: {val_ssim}\")\n",
        "\n",
        "# Plotting the training history (loss, PSNR, SSIM)\n",
        "# You can use the plotting code you've provided to visualize the training and validation metrics over epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xA9pCsmsZkg",
        "outputId": "11d46001-5cfb-41bf-9ad4-ad5630bc122b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Starting dataset creation...\n",
            "Collecting image paths...\n",
            "Directory: /content/drive/My Drive/diss/myproj/data_train/clear_images/strasbourg uses cityscapes logic.\n",
            "Directory: /content/drive/My Drive/diss/myproj/data_train/clear_images/hamburg uses cityscapes logic.\n",
            "Directory: /content/drive/My Drive/diss/myproj/data_train/clear_images/aachen uses cityscapes logic.\n",
            "Directory: /content/drive/My Drive/diss/myproj/data_train/clear_images/hanover uses cityscapes logic.\n",
            "Directory: /content/drive/My Drive/diss/myproj/data_train/reside_clear uses reside logic.\n",
            "Collected 5939 pairs of images.\n",
            "Creating dataset from paths...\n",
            "Processing: Tensor(\"args_0:0\", shape=(), dtype=string) and Tensor(\"args_1:0\", shape=(), dtype=string)\n",
            "Shuffling dataset...\n",
            "Batching dataset...\n",
            "Dataset created and ready for use.\n",
            "Sample batch - Hazy images shape: (32, 256, 256, 3), dtype: <dtype: 'float32'>\n",
            "Sample batch - Clear images shape: (32, 256, 256, 3), dtype: <dtype: 'float32'>\n",
            "Starting Epoch 1\n",
            "Epoch 1/30\n",
            "5/5 [==============================] - 29s 1s/step - loss: 5.2960 - psnr_metric: 10.7250 - ssim_metric: 0.2066 - val_loss: 3.9461 - val_psnr_metric: 11.8430 - val_ssim_metric: 0.3795\n",
            "Starting Epoch 2\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 4.0309 - psnr_metric: 11.4735 - ssim_metric: 0.2719 - val_loss: 3.8973 - val_psnr_metric: 11.6877 - val_ssim_metric: 0.3874\n",
            "Starting Epoch 3\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 3.2799 - psnr_metric: 11.9674 - ssim_metric: 0.3228 - val_loss: 3.8295 - val_psnr_metric: 11.5750 - val_ssim_metric: 0.3981\n",
            "Starting Epoch 4\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 2.6830 - psnr_metric: 12.4115 - ssim_metric: 0.3564 - val_loss: 3.7434 - val_psnr_metric: 11.4989 - val_ssim_metric: 0.4084\n",
            "Starting Epoch 5\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 1s 160ms/step - loss: 2.3376 - psnr_metric: 12.7367 - ssim_metric: 0.3874 - val_loss: 3.6540 - val_psnr_metric: 11.4584 - val_ssim_metric: 0.4181\n",
            "Starting Epoch 6\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 2.1066 - psnr_metric: 13.0303 - ssim_metric: 0.4106 - val_loss: 3.5709 - val_psnr_metric: 11.4556 - val_ssim_metric: 0.4265\n",
            "Starting Epoch 7\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.9134 - psnr_metric: 13.1654 - ssim_metric: 0.4297 - val_loss: 3.4988 - val_psnr_metric: 11.4636 - val_ssim_metric: 0.4334\n",
            "Starting Epoch 8\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.7720 - psnr_metric: 13.5265 - ssim_metric: 0.4443 - val_loss: 3.4287 - val_psnr_metric: 11.4830 - val_ssim_metric: 0.4397\n",
            "Starting Epoch 9\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 1.6180 - psnr_metric: 13.7277 - ssim_metric: 0.4625 - val_loss: 3.3632 - val_psnr_metric: 11.5151 - val_ssim_metric: 0.4454\n",
            "Starting Epoch 10\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 1.5723 - psnr_metric: 13.8034 - ssim_metric: 0.4691 - val_loss: 3.2929 - val_psnr_metric: 11.5509 - val_ssim_metric: 0.4514\n",
            "Starting Epoch 11\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 1.4594 - psnr_metric: 14.0465 - ssim_metric: 0.4860 - val_loss: 3.2261 - val_psnr_metric: 11.5943 - val_ssim_metric: 0.4568\n",
            "Starting Epoch 12\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 1.3799 - psnr_metric: 14.1960 - ssim_metric: 0.5031 - val_loss: 3.1592 - val_psnr_metric: 11.6345 - val_ssim_metric: 0.4621\n",
            "Starting Epoch 13\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 1.3615 - psnr_metric: 14.1465 - ssim_metric: 0.5053 - val_loss: 3.0942 - val_psnr_metric: 11.6706 - val_ssim_metric: 0.4670\n",
            "Starting Epoch 14\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.3151 - psnr_metric: 14.4784 - ssim_metric: 0.5182 - val_loss: 3.0335 - val_psnr_metric: 11.7226 - val_ssim_metric: 0.4709\n",
            "Starting Epoch 15\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.2465 - psnr_metric: 14.6959 - ssim_metric: 0.5249 - val_loss: 2.9838 - val_psnr_metric: 11.8096 - val_ssim_metric: 0.4737\n",
            "Starting Epoch 16\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 1.2141 - psnr_metric: 14.5586 - ssim_metric: 0.5281 - val_loss: 2.9379 - val_psnr_metric: 11.9089 - val_ssim_metric: 0.4769\n",
            "Starting Epoch 17\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.2378 - psnr_metric: 14.5095 - ssim_metric: 0.5279 - val_loss: 2.8992 - val_psnr_metric: 11.9758 - val_ssim_metric: 0.4795\n",
            "Starting Epoch 18\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.1947 - psnr_metric: 14.6196 - ssim_metric: 0.5401 - val_loss: 2.8535 - val_psnr_metric: 12.0235 - val_ssim_metric: 0.4820\n",
            "Starting Epoch 19\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.1886 - psnr_metric: 14.7519 - ssim_metric: 0.5466 - val_loss: 2.8095 - val_psnr_metric: 12.0836 - val_ssim_metric: 0.4846\n",
            "Starting Epoch 20\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.1489 - psnr_metric: 14.7000 - ssim_metric: 0.5530 - val_loss: 2.7778 - val_psnr_metric: 12.1282 - val_ssim_metric: 0.4863\n",
            "Starting Epoch 21\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.0733 - psnr_metric: 15.2269 - ssim_metric: 0.5641 - val_loss: 2.7267 - val_psnr_metric: 12.1961 - val_ssim_metric: 0.4900\n",
            "Starting Epoch 22\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.1054 - psnr_metric: 15.0031 - ssim_metric: 0.5586 - val_loss: 2.6690 - val_psnr_metric: 12.2571 - val_ssim_metric: 0.4932\n",
            "Starting Epoch 23\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 1.0991 - psnr_metric: 15.0279 - ssim_metric: 0.5656 - val_loss: 2.6039 - val_psnr_metric: 12.3716 - val_ssim_metric: 0.4986\n",
            "Starting Epoch 24\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 1.0586 - psnr_metric: 15.2038 - ssim_metric: 0.5714 - val_loss: 2.5522 - val_psnr_metric: 12.4642 - val_ssim_metric: 0.5033\n",
            "Starting Epoch 25\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 1.0748 - psnr_metric: 15.1963 - ssim_metric: 0.5750 - val_loss: 2.5273 - val_psnr_metric: 12.4496 - val_ssim_metric: 0.5035\n",
            "Starting Epoch 26\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 1.0377 - psnr_metric: 15.1260 - ssim_metric: 0.5791 - val_loss: 2.4921 - val_psnr_metric: 12.5162 - val_ssim_metric: 0.5061\n",
            "Starting Epoch 27\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.9992 - psnr_metric: 15.1832 - ssim_metric: 0.5844 - val_loss: 2.4174 - val_psnr_metric: 12.6720 - val_ssim_metric: 0.5144\n",
            "Starting Epoch 28\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.9287 - psnr_metric: 15.7913 - ssim_metric: 0.5951 - val_loss: 2.3581 - val_psnr_metric: 12.7579 - val_ssim_metric: 0.5191\n",
            "Starting Epoch 29\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.9345 - psnr_metric: 15.6287 - ssim_metric: 0.5971 - val_loss: 2.3000 - val_psnr_metric: 12.8413 - val_ssim_metric: 0.5240\n",
            "Starting Epoch 30\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.9141 - psnr_metric: 15.7194 - ssim_metric: 0.5985 - val_loss: 2.2379 - val_psnr_metric: 12.9220 - val_ssim_metric: 0.5300\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 2.2379 - psnr_metric: 12.9220 - ssim_metric: 0.5300\n",
            "Validation Loss: 2.2379345893859863, Validation PSNR: 12.921967506408691, Validation SSIM: 0.5300396084785461\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "def decode_img(img, image_size=(256, 256)):\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, image_size)\n",
        "    img /= 255.0  # Normalize to [0,1]\n",
        "    return img\n",
        "\n",
        "def process_path(clear_path, hazy_path):\n",
        "    print(f\"Processing: {clear_path} and {hazy_path}\")  # Print the paths being processed\n",
        "    clear_img = tf.io.read_file(clear_path)\n",
        "    clear_img = decode_img(clear_img)\n",
        "    hazy_img = tf.io.read_file(hazy_path)\n",
        "    hazy_img = decode_img(hazy_img)\n",
        "    return hazy_img, clear_img\n",
        "\n",
        "def create_dataset(dir_pairs, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    clear_paths, hazy_paths = [], []\n",
        "\n",
        "    print(\"Collecting image paths...\")\n",
        "    for clear_dir, hazy_dir in dir_pairs:\n",
        "        # Check if directories exist\n",
        "        if not os.path.exists(clear_dir):\n",
        "            print(f\"Directory does not exist: {clear_dir}\")\n",
        "            continue\n",
        "        if not os.path.exists(hazy_dir):\n",
        "            print(f\"Directory does not exist: {hazy_dir}\")\n",
        "            continue\n",
        "\n",
        "        for file_name in sorted(os.listdir(clear_dir)):\n",
        "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            base_name = file_name.rsplit('.', 1)[0]\n",
        "            pattern = f\"{base_name}_*\"\n",
        "            hazy_files = [f for f in os.listdir(hazy_dir) if f.startswith(pattern) and f.lower().endswith('.png')]\n",
        "\n",
        "            for hazy_file in hazy_files:\n",
        "                clear_paths.append(os.path.join(clear_dir, file_name))\n",
        "                hazy_paths.append(os.path.join(hazy_dir, hazy_file))\n",
        "\n",
        "    if not clear_paths:\n",
        "        print(\"No image pairs were collected.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset(dir_pairs, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    clear_paths, hazy_paths = [], []\n",
        "\n",
        "    print(\"Collecting image paths...\")\n",
        "    for clear_dir, hazy_dir in dir_pairs:\n",
        "        if not os.path.exists(clear_dir) or not os.path.exists(hazy_dir):\n",
        "            print(f\"Directory not found: {clear_dir} or {hazy_dir}\")\n",
        "            continue\n",
        "\n",
        "        clear_files = os.listdir(clear_dir)\n",
        "        hazy_files = os.listdir(hazy_dir)\n",
        "\n",
        "        if not clear_files or not hazy_files:\n",
        "            print(f\"No files found in directories: {clear_dir} or {hazy_dir}\")\n",
        "            continue\n",
        "\n",
        "        # Determine pairing logic based on directory\n",
        "        if 'reside_clear' in clear_dir.split('/'):\n",
        "            pairing_logic = 'reside'\n",
        "        else:\n",
        "            pairing_logic = 'cityscapes'\n",
        "        print(f\"Directory: {clear_dir} uses {pairing_logic} logic.\")\n",
        "\n",
        "        for file_name in sorted(clear_files):\n",
        "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "            if pairing_logic == 'cityscapes':\n",
        "              base_name = '_'.join(file_name.split('_')[:-1])\n",
        "              for beta in [\"0.01\", \"0.02\", \"0.005\"]:\n",
        "                  hazy_file_name = f\"{base_name}_leftImg8bit_foggy_beta_{beta}.png\"\n",
        "                  hazy_path = os.path.join(hazy_dir, hazy_file_name)\n",
        "                  if os.path.exists(hazy_path):\n",
        "                      clear_paths.append(os.path.join(clear_dir, file_name))\n",
        "                      hazy_paths.append(hazy_path)\n",
        "\n",
        "            elif pairing_logic == 'reside':\n",
        "              base_name = file_name.split('.')[0]\n",
        "              matched_hazy_files = [f for f in hazy_files if f.startswith(base_name + \"_\") and f.lower().endswith('.png')]\n",
        "\n",
        "              if not matched_hazy_files:\n",
        "                  print(f\"No hazy images matched for {file_name}\")\n",
        "                  continue\n",
        "\n",
        "              for hazy_file in matched_hazy_files:\n",
        "                  clear_paths.append(os.path.join(clear_dir, file_name))\n",
        "                  hazy_paths.append(os.path.join(hazy_dir, hazy_file))\n",
        "\n",
        "    if not clear_paths:\n",
        "        print(\"No image pairs were collected.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Collected {len(clear_paths)} pairs of images.\")\n",
        "\n",
        "    # Create a tf.data.Dataset from paths\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((clear_paths, hazy_paths))\n",
        "    print(\"Creating dataset from paths...\")\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        print(\"Shuffling dataset...\")\n",
        "        dataset = dataset.shuffle(buffer_size=len(clear_paths))\n",
        "\n",
        "    print(\"Batching dataset...\")\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    print(\"Dataset created and ready for use.\")\n",
        "    return dataset\n",
        "\n",
        "# Define the pairs of directories\n",
        "dir_pairs = [\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/strasbourg', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/strasbourg'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/hamburg', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/hamburg'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/aachen', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/aachen'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/hanover', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/hanover'),\n",
        "\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/reside_clear', '/content/drive/My Drive/diss/myproj/data_train/reside_haze')\n",
        "]\n",
        "\n",
        "\n",
        "# Create and use the TensorFlow dataset\n",
        "print(\"Starting dataset creation...\")\n",
        "dataset = create_dataset(dir_pairs)\n",
        "\n",
        "# Display some info about the dataset (optional)\n",
        "for hazy_images, clear_images in dataset.take(1):\n",
        "    print(f\"Sample batch - Hazy images shape: {hazy_images.shape}, dtype: {hazy_images.dtype}\")\n",
        "    print(f\"Sample batch - Clear images shape: {clear_images.shape}, dtype: {clear_images.dtype}\")\n",
        "\n",
        "#####################\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Clear any previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Set the mixed precision policy\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from dehaze import model, build_unet, my_loss, psnr_metric, ssim_metric  # Adjust import according to your file structure\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "#from dataset import hazy_images, clear_images_matched  # Assuming these are loaded and prepared as shown\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.image import psnr, ssim\n",
        "\n",
        "# Assuming `dataset` is your complete dataset returned from `create_dataset`\n",
        "# First, let's count the number of items in the dataset\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "full_dataset = dataset.shuffle(buffer_size=dataset_size)\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "val_dataset = full_dataset.skip(train_size)\n",
        "\n",
        "# Continue with your model definition and training as before\n",
        "# Ensure you use `train_dataset` and `val_dataset` for training and validation, respectively.\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset_from_paths(hazy_paths, clear_paths, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_paths, clear_paths))\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(hazy_paths))\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Load and prepare your data\n",
        "# Ensure datasets are TensorFlow Dataset objects correctly batched\n",
        "def prepare_tf_dataset(hazy_images, clear_images, batch_size=6):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_images, clear_images))\n",
        "    dataset = dataset.shuffle(buffer_size=len(hazy_images)).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(hazy_images))\n",
        "train_hazy, train_clear = hazy_images[:train_size], clear_images[:train_size]\n",
        "val_hazy, val_clear = hazy_images[train_size:], clear_images[train_size:]\n",
        "\n",
        "# Prepare TensorFlow datasets\n",
        "train_dataset = prepare_tf_dataset(train_hazy, train_clear, batch_size=6)\n",
        "val_dataset = prepare_tf_dataset(val_hazy, val_clear, batch_size=6)\n",
        "\n",
        "\n",
        "# Assume load_datasets returns properly prepared and normalized TensorFlow Dataset objects\n",
        "#train_dataset, val_dataset = load_datasets()\n",
        "\n",
        "# Build the model\n",
        "model = build_unet((256, 256, 3))\n",
        "\n",
        "# Define a learning rate schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True)\n",
        "\n",
        "# from tensorflow.keras.applications import VGG19\n",
        "\n",
        "# # Load VGG19 for perceptual loss\n",
        "# vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
        "# feature_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv4']\n",
        "# vgg19_features = Model(inputs=vgg19.input, outputs=[vgg19.get_layer(name).output for name in feature_layers])\n",
        "# vgg19_features.trainable = False\n",
        "\n",
        "# # Perceptual loss function\n",
        "# def perceptual_loss(y_true, y_pred):\n",
        "#     y_true_features = vgg19_features(y_true)\n",
        "#     y_pred_features = vgg19_features(y_pred)\n",
        "#     loss = 0\n",
        "#     for f_true, f_pred in zip(y_true_features, y_pred_features):\n",
        "#         loss += tf.reduce_mean(tf.square(f_true - f_pred))\n",
        "#     return loss\n",
        "\n",
        "\n",
        "# Compile the model using Mean Squared Error (MSE) as the loss function\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
        "              loss=custom_loss,  # Using built-in MSE loss\n",
        "              metrics=[psnr_metric, ssim_metric])\n",
        "\n",
        "\n",
        "\n",
        "# Custom callback for epoch printing\n",
        "class TrainingPrint(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"Starting Epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='min')\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = 30,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[ early_stopping_callback, TrainingPrint()])\n",
        "\n",
        "\n",
        "# Save the final model\n",
        "model.save('/content/drive/My Drive/diss/myproj/results/3_6VGG19LossMSEModResFog.keras')\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "val_loss, val_psnr, val_ssim = model.evaluate(val_dataset)\n",
        "print(f\"Validation Loss: {val_loss}, Validation PSNR: {val_psnr}, Validation SSIM: {val_ssim}\")\n",
        "\n",
        "# Plotting the training history (loss, PSNR, SSIM)\n",
        "# You can use the plotting code you've provided to visualize the training and validation metrics over epochs.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
