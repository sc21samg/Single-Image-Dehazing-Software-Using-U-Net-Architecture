{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olOFmIWRjmtV",
        "outputId": "a5228f4b-7507-42c1-829c-880f218334ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 256, 256, 64)         0         ['conv2d_11[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 256, 256, 64)         0         ['conv2d_12[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_11[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 128, 128, 128)        0         ['conv2d_13[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 128, 128, 128)        0         ['conv2d_14[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_13[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 64, 64, 256)          0         ['conv2d_15[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_14[0][0]']       \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 64, 64, 256)          0         ['conv2d_16[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 128, 128, 128)        131200    ['activation_15[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_transpose_2[0][0]',  \n",
            " )                                                                   'activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 128, 128, 128)        295040    ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 128, 128, 128)        0         ['conv2d_17[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 128, 128, 128)        0         ['conv2d_18[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 256, 256, 64)         32832     ['activation_17[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 256, 256, 128)        0         ['conv2d_transpose_3[0][0]',  \n",
            " )                                                                   'activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 256, 256, 64)         73792     ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 256, 256, 64)         0         ['conv2d_19[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 256, 256, 64)         0         ['conv2d_20[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 256, 256, 3)          195       ['activation_19[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1862979 (7.11 MB)\n",
            "Trainable params: 1862979 (7.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Activation, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_unet(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Contracting Path (Encoder)\n",
        "    # Block 1\n",
        "    c1 = Conv2D(64, (3, 3), padding='same')(inputs)\n",
        "    c1 = Activation('relu')(c1)\n",
        "    c1 = Conv2D(64, (3, 3), padding='same')(c1)\n",
        "    c1 = Activation('relu')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    # Block 2\n",
        "    c2 = Conv2D(128, (3, 3), padding='same')(p1)\n",
        "    c2 = Activation('relu')(c2)\n",
        "    c2 = Conv2D(128, (3, 3), padding='same')(c2)\n",
        "    c2 = Activation('relu')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    # Block 3 - example for additional depth\n",
        "    c3 = Conv2D(256, (3, 3), padding='same')(p2)\n",
        "    c3 = Activation('relu')(c3)\n",
        "    c3 = Conv2D(256, (3, 3), padding='same')(c3)\n",
        "    c3 = Activation('relu')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    # Expansive Path (Decoder)\n",
        "    # Block 1\n",
        "    u1 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c3)\n",
        "    u1 = Concatenate()([u1, c2])\n",
        "    c4 = Conv2D(128, (3, 3), padding='same')(u1)\n",
        "    c4 = Activation('relu')(c4)\n",
        "    c4 = Conv2D(128, (3, 3), padding='same')(c4)\n",
        "    c4 = Activation('relu')(c4)\n",
        "\n",
        "    # Block 2\n",
        "    u2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u2 = Concatenate()([u2, c1])\n",
        "    c5 = Conv2D(64, (3, 3), padding='same')(u2)\n",
        "    c5 = Activation('relu')(c5)\n",
        "    c5 = Conv2D(64, (3, 3), padding='same')(c5)\n",
        "    c5 = Activation('relu')(c5)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(3, (1, 1), activation='sigmoid')(c5)  # Modify the number of filters based on the number of classes\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "model = build_unet((256, 256, 3))\n",
        "\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Peak Signal-to-Noise Ratio metric.\n",
        "    \"\"\"\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def ssim_metric(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom metric for calculating the Structural Similarity Index (SSIM)\n",
        "    between two images. Ensures both inputs are float32.\n",
        "    \"\"\"\n",
        "    y_true_f32 = tf.cast(y_true, tf.float32)\n",
        "    y_pred_f32 = tf.cast(y_pred, tf.float32)\n",
        "    return tf.image.ssim(y_true_f32, y_pred_f32, max_val=1.0)\n",
        "\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "              metrics=[psnr_metric, ssim_metric])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "def decode_img(img, image_size=(256, 256)):\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, image_size)\n",
        "    img /= 255.0  # Normalize to [0,1]\n",
        "    return img\n",
        "\n",
        "def process_path(clear_path, hazy_path):\n",
        "    print(f\"Processing: {clear_path} and {hazy_path}\")  # Print the paths being processed\n",
        "    clear_img = tf.io.read_file(clear_path)\n",
        "    clear_img = decode_img(clear_img)\n",
        "    hazy_img = tf.io.read_file(hazy_path)\n",
        "    hazy_img = decode_img(hazy_img)\n",
        "    return hazy_img, clear_img\n",
        "\n",
        "def create_dataset(dir_pairs, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    clear_paths, hazy_paths = [], []\n",
        "\n",
        "    print(\"Collecting image paths...\")\n",
        "    # Collect paths of both clear and corresponding hazy images\n",
        "    for clear_dir, hazy_dir in dir_pairs:\n",
        "        # Assuming file names match except for the \"foggy_beta\" part\n",
        "        for file_name in sorted(os.listdir(clear_dir)):\n",
        "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            base_name = '_'.join(file_name.split('_')[:-1])\n",
        "            for beta in [\"0.01\", \"0.02\", \"0.005\"]:\n",
        "                hazy_file_name = f\"{base_name}_leftImg8bit_foggy_beta_{beta}.png\"\n",
        "                hazy_path = os.path.join(hazy_dir, hazy_file_name)\n",
        "                if os.path.exists(hazy_path):\n",
        "                    clear_paths.append(os.path.join(clear_dir, file_name))\n",
        "                    hazy_paths.append(hazy_path)\n",
        "\n",
        "    print(f\"Collected {len(clear_paths)} pairs of images.\")\n",
        "\n",
        "    # Create a tf.data.Dataset from paths\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((clear_paths, hazy_paths))\n",
        "    print(\"Creating dataset from paths...\")\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        print(\"Shuffling dataset...\")\n",
        "        dataset = dataset.shuffle(buffer_size=len(clear_paths))\n",
        "\n",
        "    print(\"Batching dataset...\")\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    print(\"Dataset created and ready for use.\")\n",
        "    return dataset\n",
        "\n",
        "# Define the pairs of directories\n",
        "dir_pairs = [\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/strasbourg', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/strasbourg'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/hamburg', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/hamburg'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/aachen', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/aachen'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/hanover', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/hanover')\n",
        "]\n",
        "\n",
        "# Create and use the TensorFlow dataset\n",
        "print(\"Starting dataset creation...\")\n",
        "dataset = create_dataset(dir_pairs)\n",
        "\n",
        "# Display some info about the dataset (optional)\n",
        "for hazy_images, clear_images in dataset.take(1):\n",
        "    print(f\"Sample batch - Hazy images shape: {hazy_images.shape}, dtype: {hazy_images.dtype}\")\n",
        "    print(f\"Sample batch - Clear images shape: {clear_images.shape}, dtype: {clear_images.dtype}\")\n",
        "\n",
        "#####################\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Clear any previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Set the mixed precision policy\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from dehaze import model, build_unet, my_loss, psnr_metric, ssim_metric  # Adjust import according to your file structure\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "#from dataset import hazy_images, clear_images_matched  # Assuming these are loaded and prepared as shown\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.image import psnr, ssim\n",
        "\n",
        "# Assuming `dataset` is your complete dataset returned from `create_dataset`\n",
        "# First, let's count the number of items in the dataset\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "full_dataset = dataset.shuffle(buffer_size=dataset_size)\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "val_dataset = full_dataset.skip(train_size)\n",
        "\n",
        "# Continue with your model definition and training as before\n",
        "# Ensure you use `train_dataset` and `val_dataset` for training and validation, respectively.\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset_from_paths(hazy_paths, clear_paths, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_paths, clear_paths))\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(hazy_paths))\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Load and prepare your data\n",
        "# Ensure datasets are TensorFlow Dataset objects correctly batched\n",
        "def prepare_tf_dataset(hazy_images, clear_images, batch_size=6):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_images, clear_images))\n",
        "    dataset = dataset.shuffle(buffer_size=len(hazy_images)).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(hazy_images))\n",
        "train_hazy, train_clear = hazy_images[:train_size], clear_images[:train_size]\n",
        "val_hazy, val_clear = hazy_images[train_size:], clear_images[train_size:]\n",
        "\n",
        "# Prepare TensorFlow datasets\n",
        "train_dataset = prepare_tf_dataset(train_hazy, train_clear, batch_size=6)\n",
        "val_dataset = prepare_tf_dataset(val_hazy, val_clear, batch_size=6)\n",
        "\n",
        "\n",
        "# Assume load_datasets returns properly prepared and normalized TensorFlow Dataset objects\n",
        "#train_dataset, val_dataset = load_datasets()\n",
        "\n",
        "# Build the model\n",
        "model = build_unet((256, 256, 3))\n",
        "\n",
        "# Define a learning rate schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True)\n",
        "\n",
        "# Compile the model using Mean Squared Error (MSE) as the loss function\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
        "              loss='mean_squared_error',  # Using built-in MSE loss\n",
        "              metrics=[psnr_metric, ssim_metric])\n",
        "\n",
        "\n",
        "\n",
        "# Custom callback for epoch printing\n",
        "class TrainingPrint(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"Starting Epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='min')\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = 30,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[ early_stopping_callback, TrainingPrint()])\n",
        "\n",
        "\n",
        "# Save the final model\n",
        "model.save('/content/drive/My Drive/diss/myproj/results/3_0BasicUNet.keras')\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "val_loss, val_psnr, val_ssim = model.evaluate(val_dataset)\n",
        "print(f\"Validation Loss: {val_loss}, Validation PSNR: {val_psnr}, Validation SSIM: {val_ssim}\")\n",
        "\n",
        "# Plotting the training history (loss, PSNR, SSIM)\n",
        "# You can use the plotting code you've provided to visualize the training and validation metrics over epochs.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYhVF56Nj57a",
        "outputId": "e3abf45f-0d62-4d1b-d90b-d29f8524a483"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Starting dataset creation...\n",
            "Collecting image paths...\n",
            "Collected 2949 pairs of images.\n",
            "Creating dataset from paths...\n",
            "Processing: Tensor(\"args_0:0\", shape=(), dtype=string) and Tensor(\"args_1:0\", shape=(), dtype=string)\n",
            "Shuffling dataset...\n",
            "Batching dataset...\n",
            "Dataset created and ready for use.\n",
            "Sample batch - Hazy images shape: (32, 256, 256, 3), dtype: <dtype: 'float32'>\n",
            "Sample batch - Clear images shape: (32, 256, 256, 3), dtype: <dtype: 'float32'>\n",
            "Starting Epoch 1\n",
            "Epoch 1/30\n",
            "5/5 [==============================] - 12s 469ms/step - loss: 0.0680 - psnr_metric: 11.8090 - ssim_metric: 0.4086 - val_loss: 0.0583 - val_psnr_metric: 12.4977 - val_ssim_metric: 0.4218\n",
            "Starting Epoch 2\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0633 - psnr_metric: 12.1413 - ssim_metric: 0.4082 - val_loss: 0.0537 - val_psnr_metric: 12.9413 - val_ssim_metric: 0.4189\n",
            "Starting Epoch 3\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0575 - psnr_metric: 12.6179 - ssim_metric: 0.4064 - val_loss: 0.0494 - val_psnr_metric: 13.4804 - val_ssim_metric: 0.4099\n",
            "Starting Epoch 4\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0527 - psnr_metric: 13.1269 - ssim_metric: 0.4008 - val_loss: 0.0544 - val_psnr_metric: 12.9868 - val_ssim_metric: 0.3953\n",
            "Starting Epoch 5\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0558 - psnr_metric: 12.9587 - ssim_metric: 0.4004 - val_loss: 0.0516 - val_psnr_metric: 13.2691 - val_ssim_metric: 0.4098\n",
            "Starting Epoch 6\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0520 - psnr_metric: 13.2069 - ssim_metric: 0.4110 - val_loss: 0.0483 - val_psnr_metric: 13.5983 - val_ssim_metric: 0.4190\n",
            "Starting Epoch 7\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0521 - psnr_metric: 13.1244 - ssim_metric: 0.4124 - val_loss: 0.0481 - val_psnr_metric: 13.5998 - val_ssim_metric: 0.4214\n",
            "Starting Epoch 8\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0516 - psnr_metric: 13.1657 - ssim_metric: 0.4143 - val_loss: 0.0478 - val_psnr_metric: 13.6386 - val_ssim_metric: 0.4217\n",
            "Starting Epoch 9\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0507 - psnr_metric: 13.2798 - ssim_metric: 0.4164 - val_loss: 0.0479 - val_psnr_metric: 13.6226 - val_ssim_metric: 0.4225\n",
            "Starting Epoch 10\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0503 - psnr_metric: 13.3341 - ssim_metric: 0.4191 - val_loss: 0.0474 - val_psnr_metric: 13.6649 - val_ssim_metric: 0.4260\n",
            "Starting Epoch 11\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0496 - psnr_metric: 13.3713 - ssim_metric: 0.4223 - val_loss: 0.0464 - val_psnr_metric: 13.7571 - val_ssim_metric: 0.4303\n",
            "Starting Epoch 12\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0492 - psnr_metric: 13.4202 - ssim_metric: 0.4254 - val_loss: 0.0464 - val_psnr_metric: 13.7413 - val_ssim_metric: 0.4325\n",
            "Starting Epoch 13\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0480 - psnr_metric: 13.5400 - ssim_metric: 0.4297 - val_loss: 0.0452 - val_psnr_metric: 13.8504 - val_ssim_metric: 0.4373\n",
            "Starting Epoch 14\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0469 - psnr_metric: 13.6145 - ssim_metric: 0.4339 - val_loss: 0.0444 - val_psnr_metric: 13.9160 - val_ssim_metric: 0.4425\n",
            "Starting Epoch 15\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0461 - psnr_metric: 13.7269 - ssim_metric: 0.4398 - val_loss: 0.0437 - val_psnr_metric: 13.9467 - val_ssim_metric: 0.4479\n",
            "Starting Epoch 16\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0436 - psnr_metric: 13.9436 - ssim_metric: 0.4474 - val_loss: 0.0404 - val_psnr_metric: 14.2646 - val_ssim_metric: 0.4585\n",
            "Starting Epoch 17\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0408 - psnr_metric: 14.2260 - ssim_metric: 0.4583 - val_loss: 0.0394 - val_psnr_metric: 14.2906 - val_ssim_metric: 0.4690\n",
            "Starting Epoch 18\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0363 - psnr_metric: 14.7140 - ssim_metric: 0.4746 - val_loss: 0.0343 - val_psnr_metric: 14.8136 - val_ssim_metric: 0.4860\n",
            "Starting Epoch 19\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0333 - psnr_metric: 14.9735 - ssim_metric: 0.4903 - val_loss: 0.0294 - val_psnr_metric: 15.4347 - val_ssim_metric: 0.5121\n",
            "Starting Epoch 20\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0299 - psnr_metric: 15.5289 - ssim_metric: 0.5100 - val_loss: 0.0322 - val_psnr_metric: 15.0628 - val_ssim_metric: 0.5082\n",
            "Starting Epoch 21\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.0320 - psnr_metric: 15.3480 - ssim_metric: 0.5065 - val_loss: 0.0428 - val_psnr_metric: 13.7546 - val_ssim_metric: 0.4496\n",
            "Starting Epoch 22\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.0339 - psnr_metric: 14.9188 - ssim_metric: 0.4934 - val_loss: 0.0317 - val_psnr_metric: 15.1006 - val_ssim_metric: 0.5095\n",
            "Starting Epoch 23\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0298 - psnr_metric: 15.5664 - ssim_metric: 0.5151 - val_loss: 0.0313 - val_psnr_metric: 15.1736 - val_ssim_metric: 0.5158\n",
            "Starting Epoch 24\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.0289 - psnr_metric: 15.6655 - ssim_metric: 0.5186 - val_loss: 0.0289 - val_psnr_metric: 15.5047 - val_ssim_metric: 0.5214\n",
            "Starting Epoch 25\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.0278 - psnr_metric: 15.8310 - ssim_metric: 0.5232 - val_loss: 0.0281 - val_psnr_metric: 15.6140 - val_ssim_metric: 0.5284\n",
            "Starting Epoch 26\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0269 - psnr_metric: 15.9432 - ssim_metric: 0.5276 - val_loss: 0.0282 - val_psnr_metric: 15.6278 - val_ssim_metric: 0.5336\n",
            "Starting Epoch 27\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0279 - psnr_metric: 15.8652 - ssim_metric: 0.5305 - val_loss: 0.0272 - val_psnr_metric: 15.7573 - val_ssim_metric: 0.5346\n",
            "Starting Epoch 28\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0287 - psnr_metric: 15.6856 - ssim_metric: 0.5230 - val_loss: 0.0269 - val_psnr_metric: 15.8174 - val_ssim_metric: 0.5371\n",
            "Starting Epoch 29\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0276 - psnr_metric: 15.8449 - ssim_metric: 0.5294 - val_loss: 0.0276 - val_psnr_metric: 15.7002 - val_ssim_metric: 0.5338\n",
            "Starting Epoch 30\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0261 - psnr_metric: 16.1113 - ssim_metric: 0.5352 - val_loss: 0.0284 - val_psnr_metric: 15.5781 - val_ssim_metric: 0.5337\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0284 - psnr_metric: 15.5781 - ssim_metric: 0.5337\n",
            "Validation Loss: 0.028362425044178963, Validation PSNR: 15.57812786102295, Validation SSIM: 0.5336839556694031\n"
          ]
        }
      ]
    }
  ]
}