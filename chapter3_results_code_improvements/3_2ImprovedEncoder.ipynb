{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Activation, Conv2DTranspose, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a convolutional block\n",
        "def conv_block(x, num_filters):\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Build U-Net model with added layers for 512 and 1024 filters\n",
        "def build_unet(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Contracting Path (Encoder) using conv_block\n",
        "    c1 = conv_block(inputs, 64)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "    c2 = conv_block(p1, 128)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "    c3 = conv_block(p2, 256)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "    c4 = conv_block(p3, 512)\n",
        "    p4 = MaxPooling2D((2, 2))(c4)\n",
        "    c5 = conv_block(p4, 1024)  # deepest layer with the most filters\n",
        "\n",
        "    # Expansive Path (Decoder)\n",
        "    u4 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u4 = Concatenate()([u4, c4])\n",
        "    c6 = conv_block(u4, 512)\n",
        "\n",
        "    u3 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u3 = Concatenate()([u3, c3])\n",
        "    c7 = conv_block(u3, 256)\n",
        "\n",
        "    u2 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u2 = Concatenate()([u2, c2])\n",
        "    c8 = conv_block(u2, 128)\n",
        "\n",
        "    u1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u1 = Concatenate()([u1, c1])\n",
        "    c9 = conv_block(u1, 64)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(3, (1, 1), activation='sigmoid')(c9)  # Modify the number of filters based on the number of classes\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Metrics\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    # Peak Signal-to-Noise Ratio metric\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def ssim_metric(y_true, y_pred):\n",
        "    # Structural Similarity Index\n",
        "    y_true_f32 = tf.cast(y_true, tf.float32)\n",
        "    y_pred_f32 = tf.cast(y_pred, tf.float32)\n",
        "    return tf.image.ssim(y_true_f32, y_pred_f32, max_val=1.0)\n",
        "\n",
        "# Example usage\n",
        "model = build_unet((256, 256, 3))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[psnr_metric, ssim_metric])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I52I5LBLssaM",
        "outputId": "18280ed9-3b3a-445a-b5e3-ff387800dfdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 256, 256, 64)         1792      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, 32, 32, 512)          2097664   ['activation_9[0][0]']        \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 32, 32, 1024)         0         ['conv2d_transpose[0][0]',    \n",
            "                                                                     'activation_7[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 32, 32, 512)          4719104   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, 64, 64, 256)          524544    ['activation_11[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 64, 64, 512)          0         ['conv2d_transpose_1[0][0]',  \n",
            " )                                                                   'activation_5[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 64, 64, 256)          1179904   ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 128, 128, 128)        131200    ['activation_13[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_transpose_2[0][0]',  \n",
            " )                                                                   'activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 128, 128, 128)        295040    ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 256, 256, 64)         32832     ['activation_15[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 256, 256, 128)        0         ['conv2d_transpose_3[0][0]',  \n",
            " )                                                                   'activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 256, 256, 64)         73792     ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 256, 256, 3)          195       ['activation_17[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31055427 (118.47 MB)\n",
            "Trainable params: 31043651 (118.42 MB)\n",
            "Non-trainable params: 11776 (46.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "def decode_img(img, image_size=(256, 256)):\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, image_size)\n",
        "    img /= 255.0  # Normalize to [0,1]\n",
        "    return img\n",
        "\n",
        "def process_path(clear_path, hazy_path):\n",
        "    print(f\"Processing: {clear_path} and {hazy_path}\")  # Print the paths being processed\n",
        "    clear_img = tf.io.read_file(clear_path)\n",
        "    clear_img = decode_img(clear_img)\n",
        "    hazy_img = tf.io.read_file(hazy_path)\n",
        "    hazy_img = decode_img(hazy_img)\n",
        "    return hazy_img, clear_img\n",
        "\n",
        "def create_dataset(dir_pairs, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    clear_paths, hazy_paths = [], []\n",
        "\n",
        "    print(\"Collecting image paths...\")\n",
        "    # Collect paths of both clear and corresponding hazy images\n",
        "    for clear_dir, hazy_dir in dir_pairs:\n",
        "        # Assuming file names match except for the \"foggy_beta\" part\n",
        "        for file_name in sorted(os.listdir(clear_dir)):\n",
        "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            base_name = '_'.join(file_name.split('_')[:-1])\n",
        "            for beta in [\"0.01\", \"0.02\", \"0.005\"]:\n",
        "                hazy_file_name = f\"{base_name}_leftImg8bit_foggy_beta_{beta}.png\"\n",
        "                hazy_path = os.path.join(hazy_dir, hazy_file_name)\n",
        "                if os.path.exists(hazy_path):\n",
        "                    clear_paths.append(os.path.join(clear_dir, file_name))\n",
        "                    hazy_paths.append(hazy_path)\n",
        "\n",
        "    print(f\"Collected {len(clear_paths)} pairs of images.\")\n",
        "\n",
        "    # Create a tf.data.Dataset from paths\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((clear_paths, hazy_paths))\n",
        "    print(\"Creating dataset from paths...\")\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        print(\"Shuffling dataset...\")\n",
        "        dataset = dataset.shuffle(buffer_size=len(clear_paths))\n",
        "\n",
        "    print(\"Batching dataset...\")\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    print(\"Dataset created and ready for use.\")\n",
        "    return dataset\n",
        "\n",
        "# Define the pairs of directories\n",
        "dir_pairs = [\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/strasbourg', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/strasbourg'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/hamburg', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/hamburg'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/aachen', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/aachen'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/hanover', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/hanover')\n",
        "]\n",
        "\n",
        "# Create and use the TensorFlow dataset\n",
        "print(\"Starting dataset creation...\")\n",
        "dataset = create_dataset(dir_pairs)\n",
        "\n",
        "# Display some info about the dataset (optional)\n",
        "for hazy_images, clear_images in dataset.take(1):\n",
        "    print(f\"Sample batch - Hazy images shape: {hazy_images.shape}, dtype: {hazy_images.dtype}\")\n",
        "    print(f\"Sample batch - Clear images shape: {clear_images.shape}, dtype: {clear_images.dtype}\")\n",
        "\n",
        "#####################\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Clear any previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Set the mixed precision policy\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from dehaze import model, build_unet, my_loss, psnr_metric, ssim_metric  # Adjust import according to your file structure\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "#from dataset import hazy_images, clear_images_matched  # Assuming these are loaded and prepared as shown\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.image import psnr, ssim\n",
        "\n",
        "# Assuming `dataset` is your complete dataset returned from `create_dataset`\n",
        "# First, let's count the number of items in the dataset\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "full_dataset = dataset.shuffle(buffer_size=dataset_size)\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "val_dataset = full_dataset.skip(train_size)\n",
        "\n",
        "# Continue with your model definition and training as before\n",
        "# Ensure you use `train_dataset` and `val_dataset` for training and validation, respectively.\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset_from_paths(hazy_paths, clear_paths, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_paths, clear_paths))\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(hazy_paths))\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Load and prepare your data\n",
        "# Ensure datasets are TensorFlow Dataset objects correctly batched\n",
        "def prepare_tf_dataset(hazy_images, clear_images, batch_size=6):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_images, clear_images))\n",
        "    dataset = dataset.shuffle(buffer_size=len(hazy_images)).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(hazy_images))\n",
        "train_hazy, train_clear = hazy_images[:train_size], clear_images[:train_size]\n",
        "val_hazy, val_clear = hazy_images[train_size:], clear_images[train_size:]\n",
        "\n",
        "# Prepare TensorFlow datasets\n",
        "train_dataset = prepare_tf_dataset(train_hazy, train_clear, batch_size=6)\n",
        "val_dataset = prepare_tf_dataset(val_hazy, val_clear, batch_size=6)\n",
        "\n",
        "\n",
        "# Assume load_datasets returns properly prepared and normalized TensorFlow Dataset objects\n",
        "#train_dataset, val_dataset = load_datasets()\n",
        "\n",
        "# Build the model\n",
        "model = build_unet((256, 256, 3))\n",
        "\n",
        "# Define a learning rate schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True)\n",
        "\n",
        "# Compile the model using Mean Squared Error (MSE) as the loss function\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
        "              loss='mean_squared_error',  # Using built-in MSE loss\n",
        "              metrics=[psnr_metric, ssim_metric])\n",
        "\n",
        "\n",
        "\n",
        "# Custom callback for epoch printing\n",
        "class TrainingPrint(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"Starting Epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='min')\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = 30,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[ early_stopping_callback, TrainingPrint()])\n",
        "\n",
        "\n",
        "# Save the final model\n",
        "model.save('/content/drive/My Drive/diss/myproj/results/3_2ImprovedEncoder.keras')\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "val_loss, val_psnr, val_ssim = model.evaluate(val_dataset)\n",
        "print(f\"Validation Loss: {val_loss}, Validation PSNR: {val_psnr}, Validation SSIM: {val_ssim}\")\n",
        "\n",
        "# Plotting the training history (loss, PSNR, SSIM)\n",
        "# You can use the plotting code you've provided to visualize the training and validation metrics over epochs.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYhVF56Nj57a",
        "outputId": "e2db5aa6-470f-4b80-9857-9a98c988ce1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Starting dataset creation...\n",
            "Collecting image paths...\n",
            "Collected 2949 pairs of images.\n",
            "Creating dataset from paths...\n",
            "Processing: Tensor(\"args_0:0\", shape=(), dtype=string) and Tensor(\"args_1:0\", shape=(), dtype=string)\n",
            "Shuffling dataset...\n",
            "Batching dataset...\n",
            "Dataset created and ready for use.\n"
          ]
        }
      ]
    }
  ]
}