{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olOFmIWRjmtV",
        "outputId": "a5228f4b-7507-42c1-829c-880f218334ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 256, 256, 64)         0         ['conv2d_11[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 256, 256, 64)         0         ['conv2d_12[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_11[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 128, 128, 128)        0         ['conv2d_13[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 128, 128, 128)        0         ['conv2d_14[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_13[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 64, 64, 256)          0         ['conv2d_15[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_14[0][0]']       \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 64, 64, 256)          0         ['conv2d_16[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 128, 128, 128)        131200    ['activation_15[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_transpose_2[0][0]',  \n",
            " )                                                                   'activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 128, 128, 128)        295040    ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 128, 128, 128)        0         ['conv2d_17[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 128, 128, 128)        0         ['conv2d_18[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 256, 256, 64)         32832     ['activation_17[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 256, 256, 128)        0         ['conv2d_transpose_3[0][0]',  \n",
            " )                                                                   'activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 256, 256, 64)         73792     ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 256, 256, 64)         0         ['conv2d_19[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 256, 256, 64)         0         ['conv2d_20[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 256, 256, 3)          195       ['activation_19[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1862979 (7.11 MB)\n",
            "Trainable params: 1862979 (7.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Activation, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "def build_unet(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Contracting Path (Encoder)\n",
        "    # Block 1\n",
        "    c1 = Conv2D(64, (3, 3), padding='same')(inputs)\n",
        "    c1 = Activation('relu')(c1)\n",
        "    c1 = Conv2D(64, (3, 3), padding='same')(c1)\n",
        "    c1 = Activation('relu')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    # Block 2\n",
        "    c2 = Conv2D(128, (3, 3), padding='same')(p1)\n",
        "    c2 = Activation('relu')(c2)\n",
        "    c2 = Conv2D(128, (3, 3), padding='same')(c2)\n",
        "    c2 = Activation('relu')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    # Block 3 - example for additional depth\n",
        "    c3 = Conv2D(256, (3, 3), padding='same')(p2)\n",
        "    c3 = Activation('relu')(c3)\n",
        "    c3 = Conv2D(256, (3, 3), padding='same')(c3)\n",
        "    c3 = Activation('relu')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    # Expansive Path (Decoder)\n",
        "    # Block 1\n",
        "    u1 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c3)\n",
        "    u1 = Concatenate()([u1, c2])\n",
        "    c4 = Conv2D(128, (3, 3), padding='same')(u1)\n",
        "    c4 = Activation('relu')(c4)\n",
        "    c4 = Conv2D(128, (3, 3), padding='same')(c4)\n",
        "    c4 = Activation('relu')(c4)\n",
        "\n",
        "    # Block 2\n",
        "    u2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u2 = Concatenate()([u2, c1])\n",
        "    c5 = Conv2D(64, (3, 3), padding='same')(u2)\n",
        "    c5 = Activation('relu')(c5)\n",
        "    c5 = Conv2D(64, (3, 3), padding='same')(c5)\n",
        "    c5 = Activation('relu')(c5)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(3, (1, 1), activation='sigmoid')(c5)  # Modify the number of filters based on the number of classes\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "model = build_unet((256, 256, 3))\n",
        "\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Peak Signal-to-Noise Ratio metric.\n",
        "    \"\"\"\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def ssim_metric(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom metric for calculating the Structural Similarity Index (SSIM)\n",
        "    between two images. Ensures both inputs are float32.\n",
        "    \"\"\"\n",
        "    y_true_f32 = tf.cast(y_true, tf.float32)\n",
        "    y_pred_f32 = tf.cast(y_pred, tf.float32)\n",
        "    return tf.image.ssim(y_true_f32, y_pred_f32, max_val=1.0)\n",
        "\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "              metrics=[psnr_metric, ssim_metric])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Activation, Conv2DTranspose, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a convolutional block\n",
        "def conv_block(x, num_filters):\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "# Build U-Net model\n",
        "def build_unet(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Contracting Path (Encoder) using conv_block\n",
        "    c1 = conv_block(inputs, 64)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "    c2 = conv_block(p1, 128)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "    c3 = conv_block(p2, 256)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    # Expansive Path (Decoder)\n",
        "    u1 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(p3)\n",
        "    u1 = Concatenate()([u1, c3])\n",
        "    c4 = conv_block(u1, 256)\n",
        "\n",
        "    u2 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u2 = Concatenate()([u2, c2])\n",
        "    c5 = conv_block(u2, 128)\n",
        "\n",
        "    u3 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u3 = Concatenate()([u3, c1])\n",
        "    c6 = conv_block(u3, 64)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(3, (1, 1), activation='sigmoid')(c6)  # Modify the number of filters based on the number of classes\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Metrics\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    # Peak Signal-to-Noise Ratio metric\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def ssim_metric(y_true, y_pred):\n",
        "    # Structural Similarity Index\n",
        "    y_true_f32 = tf.cast(y_true, tf.float32)\n",
        "    y_pred_f32 = tf.cast(y_pred, tf.float32)\n",
        "    return tf.image.ssim(y_true_f32, y_pred_f32, max_val=1.0)\n",
        "\n",
        "# Example usage\n",
        "model = build_unet((256, 256, 3))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[psnr_metric, ssim_metric])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I52I5LBLssaM",
        "outputId": "30ed817d-7180-45c2-a7d8-bf0cd3535a59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 256, 256, 64)         256       ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 256, 256, 64)         256       ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_13[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_15[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 64, 64, 256)          1024      ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 64, 64, 256)          1024      ['conv2d_18[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_17[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 64, 64, 256)          262400    ['max_pooling2d_5[0][0]']     \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 64, 64, 512)          0         ['conv2d_transpose_3[0][0]',  \n",
            " )                                                                   'activation_17[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 64, 64, 256)          1179904   ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_18 (Ba  (None, 64, 64, 256)          1024      ['conv2d_19[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_19 (Ba  (None, 64, 64, 256)          1024      ['conv2d_20[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2D  (None, 128, 128, 128)        131200    ['activation_19[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_transpose_4[0][0]',  \n",
            " )                                                                   'activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 128, 128, 128)        295040    ['concatenate_4[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_20[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2D  (None, 256, 256, 64)         32832     ['activation_21[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate  (None, 256, 256, 128)        0         ['conv2d_transpose_5[0][0]',  \n",
            " )                                                                   'activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)          (None, 256, 256, 64)         73792     ['concatenate_5[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, 256, 256, 64)         256       ['conv2d_23[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_22[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, 256, 256, 64)         256       ['conv2d_24[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)          (None, 256, 256, 3)          195       ['activation_23[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3902531 (14.89 MB)\n",
            "Trainable params: 3898947 (14.87 MB)\n",
            "Non-trainable params: 3584 (14.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "def decode_img(img, image_size=(256, 256)):\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, image_size)\n",
        "    img /= 255.0  # Normalize to [0,1]\n",
        "    return img\n",
        "\n",
        "def process_path(clear_path, hazy_path):\n",
        "    print(f\"Processing: {clear_path} and {hazy_path}\")  # Print the paths being processed\n",
        "    clear_img = tf.io.read_file(clear_path)\n",
        "    clear_img = decode_img(clear_img)\n",
        "    hazy_img = tf.io.read_file(hazy_path)\n",
        "    hazy_img = decode_img(hazy_img)\n",
        "    return hazy_img, clear_img\n",
        "\n",
        "def create_dataset(dir_pairs, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    clear_paths, hazy_paths = [], []\n",
        "\n",
        "    print(\"Collecting image paths...\")\n",
        "    # Collect paths of both clear and corresponding hazy images\n",
        "    for clear_dir, hazy_dir in dir_pairs:\n",
        "        # Assuming file names match except for the \"foggy_beta\" part\n",
        "        for file_name in sorted(os.listdir(clear_dir)):\n",
        "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            base_name = '_'.join(file_name.split('_')[:-1])\n",
        "            for beta in [\"0.01\", \"0.02\", \"0.005\"]:\n",
        "                hazy_file_name = f\"{base_name}_leftImg8bit_foggy_beta_{beta}.png\"\n",
        "                hazy_path = os.path.join(hazy_dir, hazy_file_name)\n",
        "                if os.path.exists(hazy_path):\n",
        "                    clear_paths.append(os.path.join(clear_dir, file_name))\n",
        "                    hazy_paths.append(hazy_path)\n",
        "\n",
        "    print(f\"Collected {len(clear_paths)} pairs of images.\")\n",
        "\n",
        "    # Create a tf.data.Dataset from paths\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((clear_paths, hazy_paths))\n",
        "    print(\"Creating dataset from paths...\")\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        print(\"Shuffling dataset...\")\n",
        "        dataset = dataset.shuffle(buffer_size=len(clear_paths))\n",
        "\n",
        "    print(\"Batching dataset...\")\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    print(\"Dataset created and ready for use.\")\n",
        "    return dataset\n",
        "\n",
        "# Define the pairs of directories\n",
        "dir_pairs = [\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/strasbourg', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/strasbourg'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/hamburg', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/hamburg'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/aachen', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/aachen'),\n",
        "    ('/content/drive/My Drive/diss/myproj/data_train/clear_images/hanover', '/content/drive/My Drive/diss/myproj/data_train/hazy_images/hanover')\n",
        "]\n",
        "\n",
        "# Create and use the TensorFlow dataset\n",
        "print(\"Starting dataset creation...\")\n",
        "dataset = create_dataset(dir_pairs)\n",
        "\n",
        "# Display some info about the dataset (optional)\n",
        "for hazy_images, clear_images in dataset.take(1):\n",
        "    print(f\"Sample batch - Hazy images shape: {hazy_images.shape}, dtype: {hazy_images.dtype}\")\n",
        "    print(f\"Sample batch - Clear images shape: {clear_images.shape}, dtype: {clear_images.dtype}\")\n",
        "\n",
        "#####################\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Clear any previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Set the mixed precision policy\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from dehaze import model, build_unet, my_loss, psnr_metric, ssim_metric  # Adjust import according to your file structure\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "#from dataset import hazy_images, clear_images_matched  # Assuming these are loaded and prepared as shown\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.image import psnr, ssim\n",
        "\n",
        "# Assuming `dataset` is your complete dataset returned from `create_dataset`\n",
        "# First, let's count the number of items in the dataset\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "full_dataset = dataset.shuffle(buffer_size=dataset_size)\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "val_dataset = full_dataset.skip(train_size)\n",
        "\n",
        "# Continue with your model definition and training as before\n",
        "# Ensure you use `train_dataset` and `val_dataset` for training and validation, respectively.\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset_from_paths(hazy_paths, clear_paths, image_size=(256, 256), batch_size=32, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_paths, clear_paths))\n",
        "    dataset = dataset.map(lambda x, y: process_path(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(hazy_paths))\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Load and prepare your data\n",
        "# Ensure datasets are TensorFlow Dataset objects correctly batched\n",
        "def prepare_tf_dataset(hazy_images, clear_images, batch_size=6):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((hazy_images, clear_images))\n",
        "    dataset = dataset.shuffle(buffer_size=len(hazy_images)).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(hazy_images))\n",
        "train_hazy, train_clear = hazy_images[:train_size], clear_images[:train_size]\n",
        "val_hazy, val_clear = hazy_images[train_size:], clear_images[train_size:]\n",
        "\n",
        "# Prepare TensorFlow datasets\n",
        "train_dataset = prepare_tf_dataset(train_hazy, train_clear, batch_size=6)\n",
        "val_dataset = prepare_tf_dataset(val_hazy, val_clear, batch_size=6)\n",
        "\n",
        "\n",
        "# Assume load_datasets returns properly prepared and normalized TensorFlow Dataset objects\n",
        "#train_dataset, val_dataset = load_datasets()\n",
        "\n",
        "# Build the model\n",
        "model = build_unet((256, 256, 3))\n",
        "\n",
        "# Define a learning rate schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True)\n",
        "\n",
        "# Compile the model using Mean Squared Error (MSE) as the loss function\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
        "              loss='mean_squared_error',  # Using built-in MSE loss\n",
        "              metrics=[psnr_metric, ssim_metric])\n",
        "\n",
        "\n",
        "\n",
        "# Custom callback for epoch printing\n",
        "class TrainingPrint(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"Starting Epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='min')\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = 30,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[ early_stopping_callback, TrainingPrint()])\n",
        "\n",
        "\n",
        "# Save the final model\n",
        "model.save('/content/drive/My Drive/diss/myproj/results/3_1ImprovedConcBlock.keras')\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "val_loss, val_psnr, val_ssim = model.evaluate(val_dataset)\n",
        "print(f\"Validation Loss: {val_loss}, Validation PSNR: {val_psnr}, Validation SSIM: {val_ssim}\")\n",
        "\n",
        "# Plotting the training history (loss, PSNR, SSIM)\n",
        "# You can use the plotting code you've provided to visualize the training and validation metrics over epochs.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYhVF56Nj57a",
        "outputId": "d63a6a21-6090-4e13-c3d2-f90d3e2633d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Starting dataset creation...\n",
            "Collecting image paths...\n",
            "Collected 2949 pairs of images.\n",
            "Creating dataset from paths...\n",
            "Processing: Tensor(\"args_0:0\", shape=(), dtype=string) and Tensor(\"args_1:0\", shape=(), dtype=string)\n",
            "Shuffling dataset...\n",
            "Batching dataset...\n",
            "Dataset created and ready for use.\n",
            "Sample batch - Hazy images shape: (32, 256, 256, 3), dtype: <dtype: 'float32'>\n",
            "Sample batch - Clear images shape: (32, 256, 256, 3), dtype: <dtype: 'float32'>\n",
            "Starting Epoch 1\n",
            "Epoch 1/30\n",
            "5/5 [==============================] - 16s 589ms/step - loss: 0.0785 - psnr_metric: 11.1957 - ssim_metric: 0.0878 - val_loss: 0.0934 - val_psnr_metric: 10.3202 - val_ssim_metric: 0.2951\n",
            "Starting Epoch 2\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0566 - psnr_metric: 12.6475 - ssim_metric: 0.1683 - val_loss: 0.1377 - val_psnr_metric: 8.6249 - val_ssim_metric: 0.2484\n",
            "Starting Epoch 3\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 0s 79ms/step - loss: 0.0458 - psnr_metric: 13.5407 - ssim_metric: 0.2376 - val_loss: 0.1945 - val_psnr_metric: 7.1284 - val_ssim_metric: 0.2176\n",
            "Starting Epoch 4\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0390 - psnr_metric: 14.3115 - ssim_metric: 0.3063 - val_loss: 0.2233 - val_psnr_metric: 6.5457 - val_ssim_metric: 0.2296\n",
            "Starting Epoch 5\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0326 - psnr_metric: 15.1525 - ssim_metric: 0.3653 - val_loss: 0.2206 - val_psnr_metric: 6.6311 - val_ssim_metric: 0.2727\n",
            "Starting Epoch 6\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0299 - psnr_metric: 15.5329 - ssim_metric: 0.4086 - val_loss: 0.1878 - val_psnr_metric: 7.3422 - val_ssim_metric: 0.3068\n",
            "Starting Epoch 7\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0263 - psnr_metric: 16.2339 - ssim_metric: 0.4419 - val_loss: 0.1384 - val_psnr_metric: 8.6599 - val_ssim_metric: 0.3517\n",
            "Starting Epoch 8\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0242 - psnr_metric: 16.4758 - ssim_metric: 0.4685 - val_loss: 0.1124 - val_psnr_metric: 9.5738 - val_ssim_metric: 0.3880\n",
            "Starting Epoch 9\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0206 - psnr_metric: 17.1918 - ssim_metric: 0.5008 - val_loss: 0.0902 - val_psnr_metric: 10.5413 - val_ssim_metric: 0.4252\n",
            "Starting Epoch 10\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0187 - psnr_metric: 17.5579 - ssim_metric: 0.5188 - val_loss: 0.0686 - val_psnr_metric: 11.7025 - val_ssim_metric: 0.4501\n",
            "Starting Epoch 11\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0194 - psnr_metric: 17.5410 - ssim_metric: 0.5204 - val_loss: 0.0629 - val_psnr_metric: 12.0775 - val_ssim_metric: 0.4564\n",
            "Starting Epoch 12\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0179 - psnr_metric: 17.8355 - ssim_metric: 0.5482 - val_loss: 0.0689 - val_psnr_metric: 11.7354 - val_ssim_metric: 0.4654\n",
            "Starting Epoch 13\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0182 - psnr_metric: 18.0553 - ssim_metric: 0.5738 - val_loss: 0.0606 - val_psnr_metric: 12.2414 - val_ssim_metric: 0.4701\n",
            "Starting Epoch 14\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0163 - psnr_metric: 18.5028 - ssim_metric: 0.5821 - val_loss: 0.0545 - val_psnr_metric: 12.6866 - val_ssim_metric: 0.4653\n",
            "Starting Epoch 15\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0182 - psnr_metric: 17.8945 - ssim_metric: 0.5730 - val_loss: 0.0512 - val_psnr_metric: 12.9606 - val_ssim_metric: 0.4604\n",
            "Starting Epoch 16\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0162 - psnr_metric: 18.6535 - ssim_metric: 0.5951 - val_loss: 0.0493 - val_psnr_metric: 13.1393 - val_ssim_metric: 0.4637\n",
            "Starting Epoch 17\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0154 - psnr_metric: 18.6054 - ssim_metric: 0.6153 - val_loss: 0.0465 - val_psnr_metric: 13.3934 - val_ssim_metric: 0.4907\n",
            "Starting Epoch 18\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0160 - psnr_metric: 18.6070 - ssim_metric: 0.6277 - val_loss: 0.0473 - val_psnr_metric: 13.3050 - val_ssim_metric: 0.4934\n",
            "Starting Epoch 19\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.0156 - psnr_metric: 18.9259 - ssim_metric: 0.6308 - val_loss: 0.0495 - val_psnr_metric: 13.1145 - val_ssim_metric: 0.4848\n",
            "Starting Epoch 20\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0149 - psnr_metric: 18.8827 - ssim_metric: 0.6342 - val_loss: 0.0444 - val_psnr_metric: 13.5746 - val_ssim_metric: 0.4958\n",
            "Starting Epoch 21\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0132 - psnr_metric: 19.3479 - ssim_metric: 0.6340 - val_loss: 0.0366 - val_psnr_metric: 14.4085 - val_ssim_metric: 0.5205\n",
            "Starting Epoch 22\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0120 - psnr_metric: 19.5564 - ssim_metric: 0.6510 - val_loss: 0.0375 - val_psnr_metric: 14.3255 - val_ssim_metric: 0.5238\n",
            "Starting Epoch 23\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0128 - psnr_metric: 19.4831 - ssim_metric: 0.6586 - val_loss: 0.0372 - val_psnr_metric: 14.3351 - val_ssim_metric: 0.5222\n",
            "Starting Epoch 24\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0144 - psnr_metric: 19.0816 - ssim_metric: 0.6533 - val_loss: 0.0327 - val_psnr_metric: 14.8985 - val_ssim_metric: 0.5288\n",
            "Starting Epoch 25\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0135 - psnr_metric: 19.4514 - ssim_metric: 0.6631 - val_loss: 0.0289 - val_psnr_metric: 15.4334 - val_ssim_metric: 0.5327\n",
            "Starting Epoch 26\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0128 - psnr_metric: 19.6256 - ssim_metric: 0.6753 - val_loss: 0.0315 - val_psnr_metric: 15.0676 - val_ssim_metric: 0.5255\n",
            "Starting Epoch 27\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0125 - psnr_metric: 19.4643 - ssim_metric: 0.6746 - val_loss: 0.0300 - val_psnr_metric: 15.3116 - val_ssim_metric: 0.5355\n",
            "Starting Epoch 28\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.0123 - psnr_metric: 19.7863 - ssim_metric: 0.6819 - val_loss: 0.0283 - val_psnr_metric: 15.5691 - val_ssim_metric: 0.5488\n",
            "Starting Epoch 29\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0105 - psnr_metric: 20.2707 - ssim_metric: 0.6914 - val_loss: 0.0299 - val_psnr_metric: 15.2792 - val_ssim_metric: 0.5503\n",
            "Starting Epoch 30\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0111 - psnr_metric: 20.0592 - ssim_metric: 0.6927 - val_loss: 0.0286 - val_psnr_metric: 15.4855 - val_ssim_metric: 0.5583\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0286 - psnr_metric: 15.4855 - ssim_metric: 0.5583\n",
            "Validation Loss: 0.028645899146795273, Validation PSNR: 15.485527992248535, Validation SSIM: 0.5582996606826782\n"
          ]
        }
      ]
    }
  ]
}